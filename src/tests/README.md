# Testing with FEVER Dataset

This directory contains test scripts that use the [FEVER (Fact Extraction and VERification) dataset](https://fever.ai/dataset/fever.html) to evaluate the performance of our fact-checking system.

## Dataset Overview

The FEVER dataset consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as **Supported**, **Refuted**, or **NotEnoughInfo**.

For more information about the dataset, visit: [https://fever.ai/dataset/fever.html](https://fever.ai/dataset/fever.html)

## Download Instructions

1. **Visit the FEVER website**: Go to [https://fever.ai/dataset/fever.html](https://fever.ai/dataset/fever.html)

2. **Download the required files**:
   - **Shared Task Development Dataset (Labelled)** - Contains 19,998 claims with labels and evidence
   - **Pre-processed Wikipedia Pages (June 2017 dump)** - Contains the Wikipedia pages used as evidence

3. **Extract the files**:
   - Extract the development dataset to get `shared_task_dev.jsonl`
   - Extract the Wikipedia pages to get the `wiki-pages` directory

## Directory Structure Setup

Create the following directory structure within your `verifact` project:

```
verifact/
├── data/
│   └── testing_data/
│       ├── shared_task_dev.jsonl          # Downloaded from FEVER website
│       └── wiki-pages/
│           └── wiki-pages/                 # Extracted from wiki-pages.tar.gz
│               ├── A/
│               ├── B/
│               ├── C/
│               └── ... (other directories)
```

## File Descriptions

### shared_task_dev.jsonl
- **Source**: [FEVER Shared Task Development Dataset](https://fever.ai/dataset/fever.html)
- **Format**: JSONL (one JSON object per line)
- **Content**: 19,998 claims with labels and evidence annotations
- **Fields**:
  - `id`: Unique claim identifier
  - `label`: "SUPPORTS", "REFUTES", or "NOT ENOUGH INFO"
  - `claim`: The text of the claim to verify
  - `evidence`: List of evidence sets with Wikipedia page references

### wiki-pages/
- **Source**: [FEVER Pre-processed Wikipedia Pages](https://fever.ai/dataset/fever.html)
- **Format**: JSONL files organized by first letter of page titles
- **Content**: Wikipedia pages with line-by-line content
- **Structure**: Each JSON object contains:
  - `id`: Page title
  - `lines`: Array of text lines from the Wikipedia page

## Test Scripts

### 1. get_test_evidence_data.py
- **Purpose**: Processes the FEVER dataset and extracts Wikipedia content for evidence
- **Input**: `shared_task_dev.jsonl` and `wiki-pages/` directory
- **Output**: `sampled_claims_with_wiki.json` (enriched claims with Wikipedia content)
- **Usage**: Run this script first to prepare the test data

### 2. test_evidence_hunter.py
- **Purpose**: Tests the evidence hunter agent on FEVER claims
- **Input**: `sampled_claims_with_wiki.json`
- **Output**: `evidence_hunter_results.json` (evidence found by our system)
- **Usage**: Run this to test evidence gathering performance

### 3. evidence_scoring.py
- **Purpose**: Evaluates evidence hunter performance against gold standard
- **Input**: `sampled_claims_with_wiki.json` and `evidence_hunter_results.json`
- **Output**: `scoring_report.json` (detailed evaluation metrics)
- **Usage**: Run this to get performance metrics

## Running the Tests

1. **Prepare the data**:
   ```bash
   cd src/tests
   python get_test_evidence_data.py
   ```

2. **Test evidence hunting**:
   ```bash
   python test_evidence_hunter.py
   ```

3. **Evaluate performance**:
   ```bash
   python evidence_scoring.py
   ```

## Expected Outputs

- `sampled_claims_with_wiki.json`: 50 randomly sampled claims with Wikipedia evidence content
- `evidence_hunter_results.json`: Evidence found by our system for each claim
- `scoring_report.json`: Detailed evaluation including:
  - Stance matching accuracy
  - Content similarity scores
  - Per-claim performance breakdown

## Data Format Examples

### Claim Example (from shared_task_dev.jsonl)
```json
{
    "id": 62037,
    "label": "SUPPORTS",
    "claim": "Oliver Reed was a film actor.",
    "evidence": [
        [
            [<annotation_id>, <evidence_id>, "Oliver_Reed", 0]
        ]
    ]
}
```

### Enriched Claim Example (sampled_claims_with_wiki.json)
```json
{
    "claim": "Oliver Reed was a film actor.",
    "label": "SUPPORTS",
    "evidence": [
        {
            "page_title": "Oliver_Reed",
            "line_idx_in_page": 0,
            "line_text": "Oliver Reed (13 February 1938 – 2 May 1999) was an English actor."
        }
    ]
}
```

## Citation

If you use the FEVER dataset in your research, please cite:

```bibtex
@inproceedings{Thorne18Fever,
    author = {Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
    title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VERification}},
    booktitle = {NAACL-HLT},
    year = {2018}
}
```

## Troubleshooting

- **File not found errors**: Ensure all files are in the correct paths as shown in the directory structure
- **Memory issues**: The Wikipedia pages can be large; ensure sufficient RAM for processing
- **Encoding issues**: All files should be UTF-8 encoded
- **Permission errors**: Ensure read/write permissions for the data directory

## Notes

- The test scripts sample 50 claims by default for faster testing
- You can modify `SAMPLE_SIZE` in `get_test_evidence_data.py` to test with more claims
- The evaluation focuses on stance matching and content similarity
- Results are saved in JSON format for further analysis 